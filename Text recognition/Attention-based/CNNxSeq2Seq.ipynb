{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "670dNtc0LR0z"
   },
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "sys.path.append('..')\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL95XLRnDLr0"
   },
   "source": [
    "# Check GPU working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nQBEPUR8C8xG"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QC47NIx6C8tx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020\n",
      "Cuda compilation tools, release 11.2, V11.2.67\n",
      "Build cuda_11.2.r11.2/compiler.29373293_0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0': raise SystemError('GPU device not found')\n",
    "print('Found GPU at:', device_name)\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_handler import create_dataset, remove_rare_chars\n",
    "DATA_PATH = '../../Dataset/trdg'\n",
    "FONT_PATH = '../../Dataset/NomNaTong-Regular.ttf'\n",
    "HEIGHT = 148\n",
    "WIDTH = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and remove records with rare characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths, labels, vocabs = create_dataset(DATA_PATH, sim2tra=True)\n",
    "img_paths, labels, vocabs = remove_rare_chars(img_paths, labels, vocabs, threshold=3)\n",
    "print('Number of images found:', len(img_paths))\n",
    "print('Number of labels found:', len(labels))\n",
    "print('Number of unique characters:', len(vocabs))\n",
    "print('Characters present:', vocabs, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import visualize_images_labels\n",
    "visualize_images_labels(img_paths, labels, font_path=FONT_PATH, text_x=WIDTH + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfSIjAuqXeqO"
   },
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bnVKRDyCX5ne"
   },
   "outputs": [],
   "source": [
    "def standardize_text(text):\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join(['[START]', text, '[END]'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W5uaG1UYyso"
   },
   "outputs": [],
   "source": [
    "text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize = standardize_text,\n",
    "    max_tokens = max_vocab_size\n",
    ")\n",
    "text_processor.adapt(targ)\n",
    "text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg-lfhdpQS98"
   },
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrmAr-X8Qjmv"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, GRU, Dense, AdditiveAttention\n",
    "from typing import Any, Tuple\n",
    "import typing\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNCLMsdLRPEh"
   },
   "source": [
    "## Shape checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-17d-LARSZy"
   },
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly(): return\n",
    "        if isinstance(names, str): names = (names,)\n",
    "\n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "\n",
    "        if rank != len(names): raise ValueError(\n",
    "            f'Rank mismatch:\\n'\n",
    "            f'\\t found {rank}: {shape.numpy()}\\n'\n",
    "            f'\\t expected {len(names)}: {names}\\n'\n",
    "        )\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int): old_dim = name\n",
    "            else: old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1): continue\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim: raise ValueError(\n",
    "                f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                f\"\\t found: {new_dim}\\n\"\n",
    "                f\"\\t expected: {old_dim}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDzJ1nEKurKI"
   },
   "source": [
    "## The Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kjp-c60luvCq"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = Embedding(self.input_vocab_size, embedding_dim)\n",
    "\n",
    "        # The GRU layer processes those vectors sequentially.\n",
    "        self.gru = GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences = True,\n",
    "            return_state = True,\n",
    "            recurrent_initializer = 'glorot_uniform'\n",
    "        )\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-mrBffINQ2T"
   },
   "source": [
    "## The Attention head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqM7Hlb-4LUs"
   },
   "source": [
    "![](https://www.tensorflow.org/text/tutorials/images/attention_equation_1.jpg)\n",
    "![](https://www.tensorflow.org/text/tutorials/images/attention_equation_2.jpg)\n",
    "![](https://www.tensorflow.org/text/tutorials/images/attention_equation_4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ru1Hl3tnNS0Q"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = Dense(units, use_bias=False)\n",
    "        self.W2 = Dense(units, use_bias=False)\n",
    "        self.attention = AdditiveAttention()\n",
    "\n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        w1_query = self.W1(query) # From Eqn. (4), `W1 @ ht`\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "        w2_key = self.W2(value) # From Eqn. (4), `W2 @ hs`\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask = [query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed_aBVjeNTO1"
   },
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K1cUZbv5Bnn"
   },
   "source": [
    "![](https://www.tensorflow.org/text/tutorials/images/attention_equation_3.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lhrr_F5W8rE"
   },
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TNivGJyNZMN"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = Embedding(self.output_vocab_size, embedding_dim)\n",
    "\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.gru = GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences = True,\n",
    "            return_state = True,\n",
    "            recurrent_initializer = 'glorot_uniform'\n",
    "        )\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        # 4. RNN output & context vector using Eqn. (3) to generate attention vector.\n",
    "        self.Wc = Dense(dec_units, activation=tf.math.tanh, use_bias=False)\n",
    "\n",
    "        # 5. Fully connected layer produces the logits for each output.\n",
    "        self.fc = Dense(self.output_vocab_size)\n",
    "    \n",
    "    def call(self, inputs:DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\n",
    "        if state is not None: \n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # 1. Lookup the embeddings\n",
    "        vectors = self.embedding(inputs.new_tokens)\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "        # 2. Process one step with the RNN\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # 3. Use the RNN output as the query   \n",
    "        # for the attention over the encoder output.\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            query = rnn_output, \n",
    "            value = inputs.enc_output, \n",
    "            mask = inputs.mask\n",
    "        )\n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        # 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "        # [ct; ht] shape: (batch t, value_units + query_units)\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "        # 5. Eqn. (3): `at = tanh(Wc @ [ct; ht])`\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "        # 6. Generates logit predictions for the next token based on the \"attention vector\".\n",
    "        logits = self.fc(attention_vector)\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "        return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGkEHLF4QL_7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fop_xS3ANx1M"
   },
   "source": [
    "## Implement the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjzfdwBvNzgx"
   },
   "outputs": [],
   "source": [
    "class AttentionOCR(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, embedding_dim, units, \n",
    "        input_text_processor, output_text_processor, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        # Build the encoder and decoder\n",
    "        self.encoder = Encoder(\n",
    "            input_text_processor.vocabulary_size(),\n",
    "            embedding_dim, units\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            output_text_processor.vocabulary_size(),\n",
    "            embedding_dim, units\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            average_loss = self._compute_loss(self, inputs)\n",
    "\n",
    "        # Apply an optimization step\n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {'loss': average_loss}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, inputs):\n",
    "        average_loss = self._compute_loss(self, inputs)\n",
    "        return {'val_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtmf3lnTomdz"
   },
   "outputs": [],
   "source": [
    "def _preprocess(self, inputs):\n",
    "    input_text, target_text = inputs \n",
    "    self.shape_checker(input_text, ('batch',))\n",
    "    self.shape_checker(target_text, ('batch',))\n",
    "\n",
    "    # Convert the text to token IDs\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    target_tokens = self.output_text_processor(target_text)\n",
    "    self.shape_checker(input_tokens, ('batch', 's'))\n",
    "    self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "    # Convert IDs to masks.\n",
    "    input_mask = input_tokens != 0\n",
    "    self.shape_checker(input_mask, ('batch', 's'))\n",
    "\n",
    "    target_mask = target_tokens != 0\n",
    "    self.shape_checker(target_mask, ('batch', 't'))\n",
    "    return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDQD63IKKW6Z"
   },
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    # Run the decoder one step.\n",
    "    decoder_input = DecoderInput(\n",
    "        new_tokens = new_tokens[:, 0:1], \n",
    "        enc_output = enc_output,\n",
    "        mask = input_mask\n",
    "    )\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "    self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "    self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "    self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "    # `self.loss` returns the total for non-padded tokens\n",
    "    y_true, y_pred = new_tokens[:, 1:2], dec_result.logits\n",
    "    step_loss = self.loss(y_true, y_pred)\n",
    "    return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW4K7DnVm2Zv"
   },
   "outputs": [],
   "source": [
    "def _compute_loss(self, inputs):\n",
    "    input_tokens, input_mask, target_tokens, target_mask = self._preprocess(inputs)\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    # Encode the input\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "    # Initialize the decoder's state to the encoder's final state.\n",
    "    # This only works if the encoder and decoder have the same number of units.\n",
    "    dec_state = enc_state\n",
    "    loss = tf.constant(0.0)\n",
    "\n",
    "    for t in tf.range(max_target_length - 1):\n",
    "        # Pass in 2 tokens from the target sequence:\n",
    "        # 1. The current input to the decoder.\n",
    "        # 2. The target for the decoder's next prediction.\n",
    "        step_loss, dec_state = self._loop_step(\n",
    "            target_tokens[:, t:t+2], input_mask,\n",
    "            enc_output, dec_state\n",
    "        )\n",
    "        loss += step_loss\n",
    "\n",
    "    # Average the loss over all non padding tokens.\n",
    "    return loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9QyzDhga4kj"
   },
   "outputs": [],
   "source": [
    "AttentionOCR._preprocess = _preprocess\n",
    "AttentionOCR._preprocess = _loop_step\n",
    "AttentionOCR._train_step = _compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIdPjn_jNj8Q"
   },
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P09enHRwNqjj"
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits = True, \n",
    "            reduction = 'none'\n",
    "        )\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, ('batch', 't'))\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        shape_checker(mask, ('batch', 't'))\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjhA7JNVN7Dz"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PE3kuUtXMspb"
   },
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rV1sfROkHiu_"
   },
   "outputs": [],
   "source": [
    "attention_ocr = AttentionOCR(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor = input_text_processor,\n",
    "    output_text_processor = output_text_processor\n",
    ")\n",
    "attention_ocr.compile(optimizer=tf.optimizers.Adam(), loss=MaskedLoss())\n",
    "attention_ocr.fit(dataset, epochs=3, callbacks=[BatchLogs('batch_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbGZVveYLEUN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmthYPxqLEG7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPb0y//yNUl2DhXDiA5S429",
   "collapsed_sections": [],
   "name": "AttentionOCR.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
